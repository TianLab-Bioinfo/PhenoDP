{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1410b0c",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“¢ Code Showcase: How to Use the Latest Version of the Summarizer\n",
    "\n",
    "In this sharing on GitHub, we will demonstrate how to use the latest version of our summarizer. This new version of the summarizer employs the output of DeepSeek-R1-671B as the target for distillation. Specifically, we use the disease entries from OMIM or Orphanet, along with their HPO annotation definitions, and input them into DeepSeek-R1-671B to generate synthetic simulated cases. For the detailed process, please refer to our manuscript.\n",
    "\n",
    "Next, we will showcase two practical functions of the summarizer:\n",
    "1. **Symptom Summary Generation**: Input the patient's HPO terms, and the summarizer will quickly generate a summary report of the patient's symptoms.\n",
    "2. **Structured Clinical Report Generation**: Demonstrate how the summarizer receives the results from the ranker and the recommender, and then generates a structured clinical report.\n",
    "\n",
    "The creator of this notebook: Baole Wen (2025.03.29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd88bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from pyhpo import Ontology\n",
    "\n",
    "ontology = Ontology(data_folder='../data/hpo-2025-05-06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf3515bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Definition(hpo_list):\n",
    "    definition_list = []\n",
    "    for t in hpo_list:\n",
    "        definition = Ontology.get_hpo_object(t).definition\n",
    "        match = re.search(r'\"(.*?)\"', definition)\n",
    "        if match:\n",
    "            definition_list.append(match.group(1))\n",
    "    return ' '.join(definition_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c5c9a",
   "metadata": {},
   "source": [
    "# HPO Terms Detected in a Patient\n",
    "\n",
    "1. **HP:0000006** - Autosomal dominant inheritance  \n",
    "2. **HP:0003593** - Infantile onset  \n",
    "3. **HP:0025104** - Capillary malformation  \n",
    "4. **HP:0001009** - Telangiectasia  \n",
    "5. **HP:0003829** - Typified by incomplete penetrance  \n",
    "6. **HP:0030713** - Vein of Galen aneurysmal malformation  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04668fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Patient_hps = ([\"HP:0000006\", \"HP:0003593\", \"HP:0025104\", \"HP:0001009\", \"HP:0003829\", \"HP:0030713\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a821c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_text = Get_Definition(Patient_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../data/model/Bio-Medical-3B-CoT-Finetuned'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "\n",
    "max_input_tokens = 2048\n",
    "\n",
    "hpo_def = Input_text\n",
    "\n",
    "prompt = f\"\"\"\n",
    "I will provide you with the definitions of some HPO (Human Phenotype Ontology) terms exhibited by a patient. Based on these definitions, please generate a concise, clinically focused summary of the patient's symptoms in one paragraph, approximately 100-300 words in length. Ensure the summary is highly readable, with smooth transitions between ideas, logical coherence, and accurate representation of the clinical features. Emphasize clarity, fluency, and clinical relevance to create a realistic and precise description of the patient's presentation.\\nText:\\n{hpo_def}\n",
    "\"\"\"\n",
    "  \n",
    "tokenized_text = tokenizer(prompt, return_tensors=\"pt\").input_ids[0]\n",
    "truncated_tokenized_text = tokenized_text[:max_input_tokens]\n",
    "    \n",
    "truncated_text = tokenizer.decode(truncated_tokenized_text)  + '<think>:\\n'\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",  \n",
    "    model=model,  \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "response = summarizer(\n",
    "    truncated_text,\n",
    "    max_new_tokens= max_input_tokens + 1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "summary = response[0]['generated_text'].split('<think>:')\n",
    "summaries.append(summary)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b31639",
   "metadata": {},
   "source": [
    "## Input prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb1be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summaries[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad75d2c",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summaries[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05834bb0",
   "metadata": {},
   "source": [
    "### Case Demonstration\n",
    "In the following case, we first used the prompt text generated in the example tutorials of the **Rank** and **Recommender**. This prompt text integrates the patient's input symptom information as well as the results from the Rank and Recommender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716595a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/case_report_prompt.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "\n",
    "max_input_tokens = 4096\n",
    "\n",
    "hpo_def = Input_text\n",
    "\n",
    "prompt = content\n",
    "  \n",
    "tokenized_text = tokenizer(prompt, return_tensors=\"pt\").input_ids[0]\n",
    "truncated_tokenized_text = tokenized_text[:max_input_tokens]\n",
    "    \n",
    "truncated_text = tokenizer.decode(truncated_tokenized_text)  + '<think>:\\n'\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",  \n",
    "    model=model,  \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "response = summarizer(\n",
    "    truncated_text,\n",
    "    max_new_tokens= max_input_tokens + 4096,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "summary = response[0]['generated_text'].split('<think>:')\n",
    "summaries.append(summary)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73009da",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac326af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summaries[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5598c8",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b025af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summaries[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274288b3",
   "metadata": {},
   "source": [
    "# Chain of thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b861117",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summaries[0][1].split('</think>')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e2167",
   "metadata": {},
   "source": [
    "# Case Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee60f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summaries[0][1].split('</think>')[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenodp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
